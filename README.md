### **MinesweeperAI â€“ AI-Powered Minesweeper Game with Deep Reinforcement Learning**


![image](https://github.com/user-attachments/assets/2ba7bf64-5cb7-439d-8602-abbf251948d6)

Overview
MinesweeperAI is a desktop-based application that combines the classic Minesweeper game with Artificial Intelligence capable of learning to play it autonomously. Developed as part of an academic project, it leverages Deep Q-Learning (DQN) to enable the agent to navigate the grid, avoid mines, and maximize rewards through trial and error. The platform is structured with clear modularity: game engine, AI model, training pipeline, and visual interface â€” ensuring extensibility and clarity for future improvements.

This project was developed during the 2024/2025 academic year as part of a Computer Science curriculum.

ğŸ¯ Features
Classic Minesweeper Gameplay: Enjoy the traditional game with a fully functioning board, flags, and mine logic.

AI-Powered Agent: The AI learns to play Minesweeper via Deep Reinforcement Learning using a Deep Q-Network (DQN).

Training Pipeline: Train the agent with train.py, including replay memory, exploration strategy, and reward shaping.

Visual TensorBoard Metrics: Real-time performance and loss monitoring via TensorBoard.

Sound & UI Feedback: Includes sound effects and visual feedback for win/loss/game progress.

Replay Memory: Saves gameplay experience for training the AI.

ğŸ—‚ Project Structure
<pre> 
  .
â”œâ”€â”€ DQN
â”‚   â”œâ”€â”€ DQN.py
â”‚   â”œâ”€â”€ DQN_agent.py
â”‚   â”œâ”€â”€ minesweeper_env.py
â”‚   â”œâ”€â”€ my_tensorboard2.py
â”‚   â”œâ”€â”€ test.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ models
â”‚   â”œâ”€â”€ replay
â”‚   â””â”€â”€ logs
â”‚       â””â”€â”€ conv64x4_dense512x2_y0.1_minlr0.001
â”œâ”€â”€ DimineurGame
â”‚   â””â”€â”€ logs
â”‚       â””â”€â”€ conv64x4_dense512x2_y0.1_minlr0.001
â”‚           â”œâ”€â”€ events.out.tfevents.1746742238.DESKTOP-2Q5NU70.12844.0.v2
â”‚           â””â”€â”€ train
â”œâ”€â”€ creationAI
â”‚   â”œâ”€â”€ creation.py
â”‚   â”œâ”€â”€ documetation.md
â”‚   â””â”€â”€ pics
â”œâ”€â”€ images
â”œâ”€â”€ logs
â”œâ”€â”€ mines
â”‚   â”œâ”€â”€ ChampMines.py
â”‚   â””â”€â”€ mine.py
â”œâ”€â”€ models
â”‚   â””â”€â”€ conv64x4_dense512x2_y0.1_minlr0.001.h5
â”œâ”€â”€ replay
â”‚   â””â”€â”€ conv64x4_dense512x2_y0.1_minlr0.001.pkl
â”œâ”€â”€ sounds
â”‚   â”œâ”€â”€ ai_move.wav
â”‚   â”œâ”€â”€ defeat.mp3
â”‚   â”œâ”€â”€ put_flag.wav
â”‚   â”œâ”€â”€ reveal_move.wav
â”‚   â””â”€â”€ victory.wav
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ ai.py
â”œâ”€â”€ constants.py
â”œâ”€â”€ grille.py
â”œâ”€â”€ main.py
â”œâ”€â”€ minesweeper.log
â”œâ”€â”€ my_tensorboard.py

</pre>

ğŸ§  How the AI Works
The AI agent uses a Deep Q-Network (DQN) with an Îµ-greedy policy. It interacts with the environment (grid), receives rewards for safe moves, and penalties for hitting mines. The Q-values are learned through backpropagation based on the agentâ€™s experience.

Core Concepts:

States: Representation of the current grid.

Actions: Choose a cell to click.

Rewards: +10 for safe click, -50 for mine, +100 for win.

Neural Network: Fully connected layers to approximate Q-values.

Replay Buffer: Store past moves for batch training.

ğŸš€ Getting Started
Prerequisites
Python 3.8+
pip
TensorFlow / Keras
pygame

NumPy

TensorBoard

Install dependencies:

pip install -r requirements.txt

ğŸ› ï¸ Future Improvements

Integrate convolutional neural networks for spatial learning.

Implement curriculum learning to train on smaller grids first.

Web-based UI using Flask or PyScript.

Save and load training checkpoints.

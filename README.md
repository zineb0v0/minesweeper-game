
# **Le jeu DÃ©mineur avec intÃ©gration de lâ€™IA**


###  **AperÃ§u**
![image](https://github.com/user-attachments/assets/edfc8046-5b1e-49e7-b336-05897fd97d70)

Ce projet combine le jeu classique de **DÃ©mineur** avec une intelligence artificielle capable d'apprendre Ã  y jouer de maniÃ¨re autonome. DÃ©veloppÃ©e dans le cadre dâ€™un projet acadÃ©mique, elle utilise lâ€™algorithme **Deep Q-Learning (DQN)** pour permettre Ã  un agent de parcourir la grille, Ã©viter les mines et maximiser ses rÃ©compenses par lâ€™apprentissage par essai-erreur. La plateforme est construite de maniÃ¨re modulaire  moteur de jeu, modÃ¨le dâ€™IA, pipeline dâ€™entraÃ®nement et interface visuelle  ce qui garantit une bonne lisibilitÃ© du code et une extensibilitÃ© pour de futures amÃ©liorations.

Ce projet a Ã©tÃ© dÃ©veloppÃ© durant lâ€™annÃ©e universitaire **2024/2025**, dans le cadre du module de Modelisation Avancee et Theorie des Graphes


###  **FonctionnalitÃ©s**

* **Jeu de DÃ©mineur classique** : Profitez du jeu traditionnel avec une grille fonctionnelle, des drapeaux et une logique complÃ¨te de gestion des mines.
* **Agent intelligent** : Lâ€™IA apprend Ã  jouer au DÃ©mineur grÃ¢ce Ã  lâ€™apprentissage par renforcement profond via un **Deep Q-Network (DQN)**.
* **Pipeline d'entraÃ®nement** : EntraÃ®nez lâ€™agent avec `train.py`, incluant une mÃ©moire de rejouabilitÃ©, une stratÃ©gie dâ€™exploration et une adaptation des rÃ©compenses.
* **Suivi via TensorBoard** : Suivi en temps rÃ©el des performances et de la perte via **TensorBoard**.
* **Feedback sonore et visuel** : Inclut des effets sonores et des retours visuels pour les victoires, les dÃ©faites et la progression du jeu.
* **MÃ©moire de rejouabilitÃ©** : Sauvegarde les expÃ©riences de jeu pour amÃ©liorer lâ€™apprentissage de lâ€™IA.


ğŸ—‚ Structure de Projet 
<pre> 
  .
â”œâ”€â”€ DQN
â”‚   â”œâ”€â”€ DQN.py
â”‚   â”œâ”€â”€ DQN_agent.py
â”‚   â”œâ”€â”€ minesweeper_env.py
â”‚   â”œâ”€â”€ my_tensorboard2.py
â”‚   â”œâ”€â”€ test.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ models
â”‚   â”œâ”€â”€ replay
â”‚   â””â”€â”€ logs
â”‚       â””â”€â”€ conv64x4_dense512x2_y0.1_minlr0.001
â”œâ”€â”€ DimineurGame
â”‚   â””â”€â”€ logs
â”‚       â””â”€â”€ conv64x4_dense512x2_y0.1_minlr0.001
â”‚           â”œâ”€â”€ events.out.tfevents.1746742238.DESKTOP-2Q5NU70.12844.0.v2
â”‚           â””â”€â”€ train
â”œâ”€â”€ creationAI
â”‚   â”œâ”€â”€ creation.py
â”‚   â”œâ”€â”€ documetation.md
â”‚   â””â”€â”€ pics
â”œâ”€â”€ images
â”œâ”€â”€ logs
â”œâ”€â”€ mines
â”‚   â”œâ”€â”€ ChampMines.py
â”‚   â””â”€â”€ mine.py
â”œâ”€â”€ models
â”‚   â””â”€â”€ conv64x4_dense512x2_y0.1_minlr0.001.h5
â”œâ”€â”€ replay
â”‚   â””â”€â”€ conv64x4_dense512x2_y0.1_minlr0.001.pkl
â”œâ”€â”€ sounds
â”‚   â”œâ”€â”€ ai_move.wav
â”‚   â”œâ”€â”€ defeat.mp3
â”‚   â”œâ”€â”€ put_flag.wav
â”‚   â”œâ”€â”€ reveal_move.wav
â”‚   â””â”€â”€ victory.wav
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ ai.py
â”œâ”€â”€ constants.py
â”œâ”€â”€ grille.py
â”œâ”€â”€ main.py
â”œâ”€â”€ minesweeper.log
â”œâ”€â”€ my_tensorboard.py

</pre>


**Comment fonctionne lâ€™IA**

Lâ€™agent IA utilise un Deep Q-Network (DQN) avec une politique Îµ-greedy. Il interagit avec lâ€™environnement (la grille), reÃ§oit des rÃ©compenses pour les coups sÃ»rs et des pÃ©nalitÃ©s lorsquâ€™il touche une mine. Les valeurs Q sont apprises par rÃ©tropropagation en fonction de lâ€™expÃ©rience de lâ€™agent.

**Concepts clÃ©s :**

* Ã‰tats : ReprÃ©sentation de la grille actuelle.
* Actions : Choisir une cellule Ã  cliquer.
* RÃ©compenses :+10 pour une victoire, -10 pour une dÃ©faite (clic sur une mine), +1 pour un clic utile (progrÃ¨s), -1 pour un clic au hasard ou sans progrÃ¨s..
* RÃ©seau de neurones : Couches entiÃ¨rement connectÃ©es pour approximer les valeurs Q.
* Replay Buffer : Stocke les coups passÃ©s pour lâ€™entraÃ®nement par batch.

**Pour commencer**
**PrÃ©requis**

* Python 3.8+
* pip
* TensorFlow / Keras
* pygame
* NumPy
* TensorBoard


**Fichier requirements.txt**

Câ€™est un fichier texte qui liste toutes les bibliothÃ¨ques Python nÃ©cessaires au projet. En exÃ©cutant la commande `pip install -r requirements.txt`, toutes ces dÃ©pendances sont automatiquement installÃ©es, ce qui facilite la configuration de lâ€™environnement de travail.

## Guide de dÃ©marrage

1. **Installer les dÃ©pendances**

Ouvre un terminal dans le dossier du projet et exÃ©cuteâ€¯:
```
pip install -r requirements.txt
```

2. **Lancer le projet**
    
pour lancer le jeu exÃ©cutez le fichier principal `main.py`

---
Nous tenons Ã  remercier Monsieur Stephen Lee pour son dÃ©pÃ´t https://github.com/sdlee94/Minesweeper-AI-Reinforcement-Learning, qui nous a vraiment aidÃ©s Ã  comprendre comment intÃ©grer le Deep Q-Network (DQN) dans notre projet de DÃ©mineur


